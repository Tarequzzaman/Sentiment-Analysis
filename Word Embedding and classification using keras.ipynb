{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Sentiment Analysis\n",
    "\n",
    "# Get IDMB Movie review data from <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\">Here</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics  import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converts all  data text file to pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'aclImdb'\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "df = pd.DataFrame()\n",
    "for f in ('test', 'train'):    \n",
    "    for l in ('pos', 'neg'):\n",
    "      \n",
    "        path = os.path.join('/Users','luo','automation_code',folder, f, l)\n",
    "        for file in os.listdir (path) :\n",
    "            with open(os.path.join(path, file),'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]],ignore_index=True)\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the  pandas datframe to movie as csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"movie_data.csv\", index=False,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you got ssl certification error while downloading nltk please comment out below code and run this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviews = df.review.str.cat(sep=' ')\n",
    "#function to split text into word\n",
    "tokens = word_tokenize(reviews)\n",
    "vocabulary = set(tokens)\n",
    "print(len(vocabulary))\n",
    "frequency_dist = nltk.FreqDist(tokens)\n",
    "sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords from tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [w for w in tokens if not w in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud().generate_from_frequencies(frequency_dist)\n",
    "plt.imshow(wordcloud)\n",
    "# plt.figure(figsize=(100,70))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train_data, test_data = train_test_split(data, test_size=0.1, random_state=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:24999, 'review'].values\n",
    "y_train = df.loc[:24999, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "test_vectors = vectorizer.transform(X_test)\n",
    "print(train_vectors.shape, test_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn MultinomialNB Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(test_vectors)\n",
    "print(accuracy_score(y_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, F1 score, Precision in MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:', accuracy_score(y_test, predicted))\n",
    "print('F1 score:', f1_score(y_test, predicted))\n",
    "print('Recall:', recall_score(y_test, predicted))\n",
    "print('Precision:', precision_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto').fit(train_vectors, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, F1 score, Precision in SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6558\n",
      "F1 score: 0.7385690414704542\n",
      "Recall: 0.9724\n",
      "Precision: 0.5953955424932648\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(y_test, predicted))\n",
    "print('F1 score:', f1_score(y_test, predicted))\n",
    "print('Recall:', recall_score(y_test, predicted))\n",
    "print('Precision:', precision_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About pipeline: \n",
    "\n",
    "Pipeline is just an abstract notion, it's not some existing ml algorithm. Often in ML tasks you need to perform sequence of different transformations (find set of features, generate new features, select only some good features) of raw dataset before applying final estimator.\n",
    "\n",
    "Here is a good example of Pipeline usage. Pipeline gives you a single interface for all 3 steps of transformation and resulting estimator. It encapsulates transformers and predictors inside, and now you can do something like:\n",
    "\n",
    "```python \n",
    "vect = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "clf = SGDClassifier()\n",
    "\n",
    "vX = vect.fit_transform(Xtrain)\n",
    "tfidfX = tfidf.fit_transform(vX)\n",
    "predicted = clf.fit_predict(tfidfX)\n",
    "\n",
    "# Now evaluate all steps on test set\n",
    "vX = vect.fit_transform(Xtest)\n",
    "tfidfX = tfidf.fit_transform(vX)\n",
    "predicted = clf.fit_predict(tfidfX)\n",
    "```\n",
    "    \n",
    "With just:\n",
    "```python \n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "predicted = pipeline.fit(Xtrain).predict(Xtrain)\n",
    "```\n",
    "\n",
    "# Now evaluate all steps on test set\n",
    "predicted = pipeline.predict(Xtest)\n",
    "With pipelines you can easily perform a grid-search over set of parameters for each step of this meta-estimator. As described in the link above. All steps except last one must be transforms, last step can be transformer or predictor. Answer to edit: When you call pipln.fit() - each transformer inside pipeline will be fitted on outputs of previous transformer (First transformer is learned on raw dataset). Last estimator may be transformer or predictor, you can call fit_transform() on pipeline only if your last estimator is transformer (that implements fit_transform, or transform and fit methods separately), you can call fit_predict() or predict() on pipeline only if your last estimator is predictor. So you just can't call fit_transform or transform on pipeline, last step of which is predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Based on an actual story, John Boorman shows t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a gem. As a Film Four production - the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I really like this show. It has drama, romance...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is the best 3-D experience Disney has at ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Of the Korean movies I've seen, only three had...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Based on an actual story, John Boorman shows t...          1\n",
       "1  This is a gem. As a Film Four production - the...          1\n",
       "2  I really like this show. It has drama, romance...          1\n",
       "3  This is the best 3-D experience Disney has at ...          1\n",
       "4  Of the Korean movies I've seen, only three had...          1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/luo/automation_code/movie_data.csv') \n",
    "#here i use my local path  /Users/luo/automation_code/movie_data.csv   \n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(df, test_size=0.1, random_state=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_body = train_data['review']\n",
    "train_sentiment = train_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_body = train_data['review']\n",
    "test_sentiment = train_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text_clf_svm = Pipeline([\n",
    "    \n",
    "    ('vect', CountVectorizer(stop_words=stop_words,\n",
    "                            analyzer=\"word\", \n",
    "                            token_pattern=\"[\\S]*\",\n",
    "                            tokenizer=None,\n",
    "                            preprocessor=None)),                 \n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf-svm', SGDClassifier(loss = 'modified_huber', penalty='l2', max_iter=47, random_state=1000)),\n",
    "                                                                        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luo/tutorial-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [''] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "clf = text_clf_svm.fit(list(train_body), list(train_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(test_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9052\n",
      "F1 score: 0.9061881514711705\n",
      "Recall: 0.916221985058698\n",
      "Precision: 0.8963717045157922\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(test_sentiment, predicted))\n",
    "print('F1 score:', f1_score(test_sentiment, predicted))\n",
    "print('Recall:', recall_score(test_sentiment, predicted))\n",
    "print('Precision:', precision_score(test_sentiment, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embadading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
